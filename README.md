# FRAIT (Framework &amp; Implementation of AI Tools)

## Funding - Particpannts
The ‘FRAIT’ project was funded by the Belgian Government with the Data Innovation project.

- Start date: 1/7/2024
- End date: 31/12/2025
- Coordinating hospital: UZ Gent
- Participating hospitals: AZ Sint Lucas Gent, AZ Oudenaarde, Huisartsenvereniging Gent, Huisartsenkring Schelde-Leie

- Project team:
  - Mathias Syx
  - Mieke Deschepper
  - Helga Rogge
  - Kirsten Colpaert


## Primary Goal

The primary goal of FRAIT is to enable safe and effective use of Large Language Models (LLMs) in the medical sector by developing an advanced prompt engineering tool that generates tailored, accurate prompts for diverse clinical contexts. This includes:

-  **Develop a Human-Centered Framework for Prompt Creation**
Create and validate a structured, replicable process for generating individualized prompts that guide large language models (LLMs) in producing discharge summaries tailored to the specific needs of healthcare providers. 

- **Enhance Clinical Decision Support**
Provide clear, structured, and trustworthy summaries of patient records to reduce complexity, minimize errors, and support better clinical decision-making. 

- **Co-Creation with Healthcare Professionals**
Engage clinicians through workshops and questionnaires to define ideal summary formats, ensuring usability and alignment with real-world workflows. 
- **Evaluate LLM Performance in Healthcare Context**
Systematically assess the accuracy, reliability, and interpretability of AI-generated summaries using a dedicated evaluation framework and expert consensus. 

- **Ensure Compliance and Ethical Standards**
Implement GDPR-compliant processes, synthetic data protocols, and secure infrastructure to guarantee responsible and safe use of AI in clinical environments

## Key Outcomes and Insights

- **Concrete Workflow for Prompt Creation**
Developed a structured process to generate individualized prompts for summarizing medical discharge letters, ensuring consistency and adaptability across different clinical contexts.

- **Evaluation Protocol and Tool**
Designed and implemented a robust evaluation framework, including a dedicated tool, to assess the quality and accuracy of generated summaries.

- **Integration Readiness**
The generated summaries have reached a level of quality sufficient for integration into dedicated software tools, paving the way for real-world application.
- **Co-Creation as a Differentiator**
Active involvement of healthcare professionals in co-creation and evaluation was a cornerstone of the project—setting it apart from existing literature and other initiatives that often lack this participatory approach.
- **Continuous Evaluation Across Phases**
Established the principle that evaluation is not a one-time activity: it must occur during both the implementation and production phases to maintain quality and trust.

### Additional Insights
- **User-Centric Design Matters**
Workshops and dynamic feedback loops highlighted the importance of tailoring tools to real clinical workflows rather than theoretical models.
- **Prompt Engineering Requires Context**
Feedback revealed that optimizing prompts demands richer contextual data and “golden truth” references for validation.
- **Collaboration Accelerates Innovation**
Strong partnerships with technology vendors and local coordinators enabled rapid iteration and problem-solving, demonstrating the value of structured governance combined with informal communication channels.

# Codes Availability

## Part 1: From Workshop to Prompt
Full text available at DOI:10.2196/preprints.80613

## Part 2: Expert Evaluation and Consensus
Full text available at DOI: